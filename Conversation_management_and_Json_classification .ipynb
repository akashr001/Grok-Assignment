{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brLmWeElIp2x",
        "outputId": "06d8e5af-927c-4fec-ad38-4b50e693b64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Conversation Management & Summarization\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize Groq API client\n",
        "client = OpenAI(\n",
        "    api_key=\"Your grok API key\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "\n",
        "# Conversation history\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        "]\n",
        "\n",
        "# Settings\n",
        "K = 3               # Summarize after every 3 messages\n",
        "MAX_TURNS = 5       # Keep last 5 messages\n",
        "run_count = 0       # Count user messages\n",
        "\n",
        "# Function to summarize conversation\n",
        "def summarize_history(history):\n",
        "    prompt = \"Summarize this conversation concisely:\\n\\n\"\n",
        "    for msg in history:\n",
        "        prompt += f\"{msg['role']}: {msg['content']}\\n\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Function to keep only last MAX_TURNS messages\n",
        "def truncate_history(history):\n",
        "    return history[-MAX_TURNS:]\n",
        "\n",
        "# Function to chat\n",
        "def chat_with_groq(user_input):\n",
        "    global run_count, conversation_history\n",
        "\n",
        "    # Add user message\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Increment message count\n",
        "    run_count += 1\n",
        "\n",
        "    # Summarize every K messages\n",
        "    if run_count % K == 0:\n",
        "        summary = summarize_history(conversation_history)\n",
        "        conversation_history = [{\"role\": \"system\", \"content\": \"Summary: \" + summary}]\n",
        "\n",
        "    # Get assistant reply\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=conversation_history\n",
        "    )\n",
        "\n",
        "    assistant_reply = response.choices[0].message.content\n",
        "\n",
        "    # Add assistant reply to history\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "\n",
        "    # Keep last MAX_TURNS messages\n",
        "    conversation_history = truncate_history(conversation_history)\n",
        "\n",
        "    return assistant_reply\n",
        "\n",
        "# Test the chat\n",
        "print(\"Assistant:\", chat_with_groq(\"Hi, how are you?\"))\n",
        "print(\"Assistant:\", chat_with_groq(\"Tell me a fun fact about space.\"))\n",
        "print(\"Assistant:\", chat_with_groq(\"Can you summarize our chat later?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo1F9jkqKrkB",
        "outputId": "320cdf0e-4c74-4fc2-b886-fa9b870cdb56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: I'm doing well, thank you for asking. I'm here to assist you with any questions or tasks you may have, so please feel free to ask me anything. How can I help you today?\n",
            "Assistant: Here's a fun fact: \n",
            "\n",
            "Did you know that there is a giant storm on Jupiter that has been raging for at least 187 years? It's called the Great Red Spot. This massive storm is a huge anticyclonic storm, meaning that it's a high-pressure region with clockwise rotation. It's so large that three Earths could fit inside it. Astronomers continue to monitor the Great Red Spot, which remains one of the most fascinating and enduring features in our solar system.\n",
            "Assistant: I'm glad we're starting a new conversation from scratch. However I must note that since our previous conversation ended, I have to rely on a general recall from your most recent conversation history. Based on your request, this is a summary of our conversation so far:\n",
            "\n",
            "We started by exchanging pleasantries - I asked how you were doing, and you replied that you were doing well. Then, you asked how you could assist me. I didn't have any specific tasks or requests at that time, so our conversation flowed naturally from there.\n",
            "\n",
            "You requested a fun fact about space next, and I shared information about Jupiter's Great Red Spot. Later, you asked me to summarize our conversation, which I've attempted to do here.\n",
            "\n",
            "If you'd like to continue our conversation, I'm happy to chat with you about space or any other topic you're interested in!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: JSON Schema Classification & Extraction\n",
        "\n",
        "\n",
        "# Define the JSON schema we want to extract\n",
        "info_schema = {\n",
        "    \"name\": \"string\",\n",
        "    \"email\": \"string\",\n",
        "    \"phone\": \"string\",\n",
        "    \"location\": \"string\",\n",
        "    \"age\": \"integer\"\n",
        "}\n",
        "\n",
        "# Function to extract info\n",
        "def extract_info(user_input):\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following details in JSON format according to this schema:\n",
        "    {info_schema}\n",
        "    Chat: {user_input}\n",
        "    \"\"\"\n",
        "\n",
        "    # Send request to Groq API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    # Return the JSON output from assistant\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Example chat\n",
        "sample_chat = \"Hello, my name is Akash R, I live in Bangalore, my email is akash@example.com, phone 9876543210, age 21.\"\n",
        "\n",
        "# Extract info\n",
        "print(extract_info(sample_chat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HtkSvZhZVZN",
        "outputId": "0b5e98f2-0d62-4e78-9ede-6b3b21c9546b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's how you can extract the details in JSON format according to the schema:\n",
            "\n",
            "```python\n",
            "import json\n",
            "\n",
            "# Extracted details\n",
            "name = \"Akash R\"\n",
            "email = \"akash@example.com\"\n",
            "phone = \"9876543210\"\n",
            "location = \"Bangalore\"\n",
            "age = 21\n",
            "\n",
            "# Create a dictionary\n",
            "details = {\n",
            "    'name': name,\n",
            "    'email': email,\n",
            "    'phone': phone,\n",
            "    'location': location,\n",
            "    'age': age\n",
            "}\n",
            "\n",
            "# Convert the dictionary to JSON and print it\n",
            "json_details = json.dumps(details, indent=4)\n",
            "print(json_details)\n",
            "```\n",
            "\n",
            "Here's the output:\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"name\": \"Akash R\",\n",
            "    \"email\": \"akash@example.com\",\n",
            "    \"phone\": \"9876543210\",\n",
            "    \"location\": \"Bangalore\",\n",
            "    \"age\": 21\n",
            "}\n",
            "```\n",
            "\n",
            "This Python script first defines the extracted details as variables, then creates a dictionary (`details`) with those variables as its values. Finally, it uses the `json.dumps()` function to convert the dictionary to a JSON string and prints it out with an indentation of 4 for better readability.\n"
          ]
        }
      ]
    }
  ]
}